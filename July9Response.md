# July 9th, 2020 Lecture Response
 1. TensorFlow Hub is "a library of reuable machine learning modules" per its website. It advertizes itself as being suited fro transfer learning, which is the reuse of knowledge gained from one problem for a similar problem. We use a model from TF-hub which has been pretrained, allowing transfer learning to occur with our IMDB data and allowing for us to forgo text preprocessing. 
 2. The loss function is "binary_crossentropy" because our model is meant to output either "good" or "bad" (a binary choice). We are using "adam" as an optimizer, which I was unable to find much information that was clear to me about. Like stochastic gradient dissent, adam also has a stochastic element. Our model ended up being around 85% accurate (with more advanced aproaches being around 10% superior according to the tutorial), which is pretty good considering this is our starting point.
 3. ![Loss per Epoch](/DATA310_Images/lecture792000.png)
    - This graphs shows the amount of loss (i.e.: how "wrong" the model is per the loss function) at each epoch for the training data and the validation data. The training data (the data on which the model is fit) has its loss decrease with each iteration, while the validation data (the data which tests how the fitted model performs on data it hasn't seen) has its loss decrease until epoch 15 (where it slowly begins to increase). The training loss consistently decreases because the model is more and more fit to the training data with each epoch because of the optimizer function. The validation loss does not perpetually decrease because eventually the model fits itself to attributes of the training data that are not generalizable (it becomes overfit).
 4. ![](/DATA310_Images/lecture792001.png)
    - We also see the model becoming overfit in the accuracy graph. This graph measures how the sentiments attached to each review (good/bad) matched the actual listed sentiments of the training and validation data. The training accuracy increases with each iteration like before, and the validation accuracy initially increases before effectively stalling at about 10 epochs. 
