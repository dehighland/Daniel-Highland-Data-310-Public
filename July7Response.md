# July 7th, 2020 Lecture Response
  1. The episode introduces machine learning as answers and data generating rules/relationships instead of a programmer figuring out rules to compute answers. This means that activity that is hard to find a set of rules for as a programmer can be modeled by a network of "neurons" and then can generate predictions based on that network. He demonstrates this idea by setting + up a single neuron machine learner which is used to generate the function of a line from a list of values derived from said function. Having only a signle neuron presents a very + simple situation where we see only one rule that presumably will lead to a lot of rules being generated from a large set of neurons interconnected. 
  2. Maroney's simple model for predicted the output of a linear function yields 22.00048 and 22.002514 when 7.0 is input. They are different because there is some random element to machine learning.
     - My understanding is that the gradient dissent that informs the next iteration/epoch of the model does not move down the gradient perfectly/deterministically, as the name "stochastic" gradient dissent implies.
  3. Based off of the model Maroney's exercise outlines, only the house on Holly Point Rd (file name hudgins) is a good deal (3 bedrooms is predicted to mean the house is around 200k, while this house is selling at near 100k). The worst deal is the house on Church St (file name church), which sells at 399k, about 150k above its modeled price.
  - ![](/DATA310_Images/lecture772000.png)
